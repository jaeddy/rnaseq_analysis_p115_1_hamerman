---
title: "P115-1 Analysis"
author: "James Eddy"
date: "July 31, 2015"
runtime: shiny
output: 
  html_document:
    css: custom.css
---

```{r global_opts, echo=FALSE}
knitr::opts_chunk$set(fig.width=8, fig.height=4, fig.align='center',
                      echo=TRUE, warning=FALSE, message=FALSE)
knitr::opts_knit$set(root.dir = "..")

## numbers >= 10^5 will be denoted in scientific notation,
## and rounded to 2 digits
options(scipen = 1, digits = 2)
```

### Set up R environment

#### Load packages

First, load `required` packages for data munging, visualization, and analysis
(these are largely Hadley Wickham libraries, plus some Bioconductor tools).

```{r load_packages}
# Load my go-to libraries
library(dplyr)
library(ggplot2)
library(ggthemes)
library(stringr)
library(readr)
library(readxl)
library(reshape2)

# ..plus some others I'll need here
library(edgeR)
library(limma)
library(ggfortify)

# packages for R markdown stuff
library(knitr)
library(shiny)
```

\  

#### Define functions

Functions for plotting metrics are contained in `metric_qc_functions.R`.

```{r source_functions}
source("R/metric_qc_functions.R")
```

This is a function written by Elizabeth Whalen (shared by Michael Mason) that
might come in handy with some steps of the analysis. I've modified the fucntion
slightly, such that library sizes are updated and normalization factors are
calculated *after* filtering genes.

```{r whalen_functions}
# Function to build DGEList object, filter genes by keeping only those having % 
# samples with at least N counts, and computes normalization from library sizes
setUpDGEList <- function(countData, filterCount = 1, filterPercentage = 0.1)
{
	d <- DGEList(counts = countData)
	# d <- calcNormFactors(d) # moved further down
    
	# Filter all genes that do not have at least 'filterCount' counts per 
	# million in at least 'filterPercentage' percent of libraries
	keepRows <- rowSums(round(cpm(d$counts)) >= filterCount) >= 
	    filterPercentage*ncol(countData)
	print(table(keepRows))

	curDGE <- d[keepRows, ]
	
	# James: I've added this change so that library sizes and normalization
	# factors will always be updated/calculated after filtering genes
	
	# reset library sizes
	curDGE$samples$lib.size <- colSums(curDGE$counts)
	
	# calculate normalization factors (effective library size = 
	# lib.size * norm.factor)
	curDGE <- calcNormFactors(curDGE)
	return(curDGE)
}
```

\  

#### Load data

Next, load in counts and metrics data for the project, as well as sample
annotation for project libraries.

```{r load_data}
# Read CSV file with read counts
countFile <- "data/HMMF2ADXX_combined_counts_wMgiSymbol.csv"
countDat <- read_csv(countFile) # 37991 obs. of  18 variables
# str(countDat)

# Read CSV file with RNAseq/alignment metrics
metricFile <- "data/HMMF2ADXX_combined_metrics.csv"
metricDat <- read_csv(metricFile) # 16 obs. of  71 variables
# str(metricDat)

# Read XLSX file with sample annotation
designFile <- "data/JMD119 Sample Information .xlsx"
designDat <- read_excel(designFile, skip = 1) # 36 obs. of 18 variables
# str(designDat)
```

\  

#### Clean data

I need to do a bit of cleaning/formatting with variable names (column headers)
to make life easier and avoid breaking downstream functions.

```{r clean_data}
# Separate gene counts and gene symbols into separate objects, reformat
# variable names in countDat to only include libID
gene <- countDat$mgiSymbol
countDat <- countDat %>% 
    select(-geneName, -mgiSymbol)
names(countDat) <- names(countDat) %>% 
    str_extract("lib[0-9]+")

# Reformat variable names in metrics data frame
names(metricDat) <- names(metricDat) %>% 
    str_to_lower() %>%  # change variable names to lower case
    make.unique(sep = "_") # de-dup variable names
names(metricDat)[1] <- "lib_id" # reformat libID variable name

# Reformat row names in metrics dataframe
metricDat <- metricDat %>% 
    mutate(lib_id = str_extract(lib_id, "lib[0-9]+"))

# Reformat variable names in design data frame
names(designDat) <- names(designDat) %>% 
    str_replace_all(" +", "_") %>% # replace spaces with underscores
    str_replace_all("#", "num") %>%  # replace # with 'num'
    str_replace_all("/", "_per_") %>% 
    str_replace_all("(\\(|\\))", "") %>% # remove parentheses
    str_to_lower() %>% # change to lower case
    str_replace("(?<=(lib))[a-z]+", "") %>% # replace 'library' with 'lib'
    make.unique(sep = "_") # de-dup variable names

# Remove empty rows from design data frame
designDat <- designDat %>% 
    filter(!is.na(lib_id))
```

I'll also create a new object to store the salient information about groups in
the study I might want to compare.

```{r get_groups}
groupDat <- designDat %>% 
    # extract knockout status (WT or BCAP) and HSC population (long or short'
    # term); combine into a single group vector
    mutate(koStatus = as.factor(tolower(str_extract(sample_name, "WT|BCAP"))),
           hscPop = as.factor(tolower(str_extract(hsc_population, 
                                                  "Long|Short"))),
           group = as.factor(str_c(koStatus, hscPop, sep = "_"))) %>% 
    select(libID = lib_id,
           koStatus, hscPop, group)
```

For reference, here are the relevant groups in the data:

`r kable(groupDat)`

-----

### Inspect data

#### Plot metrics

Next, I'll look at a few standard metrics to see whether any libraries should
be excluded due to quality reasons.

```{r summarize_metrics}
# Pull out and format the subset of metrics to plot; melt data frame for plotting
metricSummary <- metricDat %>% 
    mutate(percentDuplication = unpaired_read_duplicates / 
               unpaired_reads_examined) %>% 
    select(libID = lib_id, 
           medianCVcoverage = median_cv_coverage, 
           fastqTotalReads = fastq_total_reads, 
           percentAligned = mapped_reads_w_dups,
           percentDuplication)
```

Cutoffs are set by default to standard values used in the Bioinformatics Core
for three metrics; libraries are considered to have 'failed' QC for the 
following conditions:  
* < 1 million total FASTQ reads  
* < 80% aligned reads  
* > 1.0 median CV coverage

I can also adjust slider bars to look at different QC cutoffs (red lines) for 
the x- and y-axis; dashed lines indicate outlier limits (1.5*IQR).

\  

##### Percent Aligned

```{r plot_perc_alingned_shiny, echo=FALSE}
sliderInput("alignCutoff", "percent aligned cutoff:", 
            min = 0, max = 1, step = 0.01,
            value = 0.8)
sliderInput("readsCutoff", "total FASTQ reads cutoff:", 
            min = 0, max = 5e7, step = 1e5,
            value = 1e6)

renderPlot({
    yRange <-c(input$alignCutoff, 1)
    xRange <- c(input$readsCutoff, max(metricSummary$fastqTotalReads) + 1e6)
    
    metricSummary %>%
        plot_metric("percentAligned", yRange, xRange)
})
```

***Percent aligned:***  
Percentage of aligned reads is well above the 80% cutoff for all libraries, 
with rates in the mid-90s across the board. **lib7422** is outside the nominal
outlier range, but still has `r filter(metricSummary, libID == "lib7422") %>% 
select(percentAligned) * 100`% alignment.

***Total reads:***  
While all libraries had well over 10 million reads in the input FASTQ file 
(after adapter trimming), **lib7418** appears to be quite a bit smaller than
average the average of 
`r summarise(metricSummary, mean(fastqTotalReads)) / 1e6` million reads, with 
only `r filter(metricSummary, libID == "lib7418") %>% select(fastqTotalReads) /
1e6` million reads.

\  

##### Median CV Coverage

```{r plot_med_cv_cov_shiny, echo=FALSE}
sliderInput("covCutoff", "median CV coverage cutoff:", 
            min = 0, max = 3, step = 0.01,
            value = 1)
sliderInput("readsCutoff", "total FASTQ reads cutoff:", 
            min = 0, max = 5e7, step = 1e5,
            value = 1e6)

renderPlot({
    yRange <- c(0, input$covCutoff)
    xRange <- c(input$readsCutoff, max(metricSummary$fastqTotalReads) + 1e6)
    
    metricSummary %>%
        plot_metric("medianCVcoverage", yRange, xRange)
})
```

***Median CV coverage:***  
All libraries look good (with `medianCVcoverage` generally close to 0.5) in
terms of gene coverage among the top 1000 transcripts.

\  

#### Examine data

I'll use the function defined above to build the `DGEList` object for the data,
which is the input for downstream functions.

```{r build_dge_list}
# Filter genes with (cpm > 1) in < 5% of samples
dge = setUpDGEList(countData = countDat, 
                      filterCount = 1, 
                      filterPercentage = 0.20)

str(dge)
```

Keeping only those genes with >= 1 counts per million in at least 20% (`r 0.2 *
nrow(groupDat)` samples), we're left with `r nrow(dge)` genes.

##### Sanity check

To verify the effect of the change I made to Elizabeth's code above, I'll plot
`norm.factors` and effective library size (`lib.size.eff`) under two 
scenarios:

* **preFilter:** `norm.factors` are calculated before genes are filtered
* **postFilter:** `norm.factors` are calculated after genes are filtered and
library sizes are updated

```{r sanity_check, echo=FALSE}
# pre-filter calculation of norm.factors
d1 <- DGEList(countDat)
d1 <- calcNormFactors(d1)

# initialize data frame to store results from both scenarios
normFactorTest <- d1$samples %>% 
    add_rownames(var = "libID") %>% 
    mutate(test = "preFilter") %>% 
    select(-group)

# post-filter calculation of norm.factors
filterCount <- 1
filterPercentage <- 0.2
d2 <- DGEList(countDat)
keepRows <- rowSums(round(cpm(d2$counts)) >= filterCount) >= 
    filterPercentage*ncol(countDat)
d2 <- d2[keepRows, ]
d2$samples$lib.size <- colSums(d2$counts)
d2 <- calcNormFactors(d2)

# update combined data frame
normFactorTest <- d2$samples %>% 
    add_rownames(var = "libID") %>% 
    mutate(test = "postFilter") %>% 
    select(-group) %>% 
    bind_rows(normFactorTest, .)

# plot norm.factors and lib.size.eff for both scenarios
normFactorTest %>% 
    mutate(test = relevel(as.factor(test), "preFilter"),
           lib.size.eff = norm.factors * lib.size) %>% 
    melt(measure.vars = c("norm.factors", "lib.size.eff")) %>% 
    ggplot(aes(x = libID, y = value)) +
    geom_point(aes(fill = test), shape = 21, size = 3,
               colour = "white", alpha = 0.7,
               position = position_jitter(width = 0.1, height = 0)) +
    facet_wrap(~ variable, scales = "free_y") +
    theme_classic() +
    theme(axis.text.x = element_text(angle = 90),
          panel.grid.major = element_line(colour = "grey", linetype = 3)) +
    scale_fill_colorblind()
```

For the not-to-stringent threshold used to filter genes (CPM > 1 in 20% of
samples), the order of operations for calculating `norm.factors` appears to
have minimal impact on effective library sizes.

#### Plot data

```{r}
pca <- prcomp(t(dge$counts))
autoplot(pca, data = groupDat, colour = 'group', shape = FALSE) +
    scale_color_colorblind()

confoundDat <- designDat %>% 
    select(libID = lib_id, sort_date, age_in_weeks, ng_per_ul) %>% 
    left_join(metricSummary, by = c("libID" = "libID")) %>% 
    bind_cols(groupDat %>% select(-libID))

pc1Dat <- bind_cols(confoundDat, melt(pca$x[, 1], value.name = "pc1"))
ggpairs(pc1Dat)
```

