---
title: "P115-1 Analysis"
author: "James Eddy"
date: "August 21, 2015"
output: 
  html_document: 
    css: custom.css
    highlight: pygments
    number_sections: yes
---

```{r global_opts, echo=FALSE, cache=FALSE}
knitr::opts_chunk$set(fig.width=8, fig.height=4, fig.align='center',
                      echo=TRUE, warning=FALSE, message=FALSE,
                      cache=TRUE, autodep=TRUE)
# knitr::opts_knit$set(root.dir = "..")

## numbers >= 10^5 will be denoted in scientific notation,
## and rounded to 2 digits
options(scipen = 1, digits = 2)
```

# Summary

The goal of this analysis was to investigate differential expression in mice
long- and short-term hematopoietic stem cells (HSCs) with and without knockout
of **BCAP**. Raw RNA-seq data was previously processsed and aligned to the
**GRCm37/mm9** reference genome with **`bowtie`** and **`TopHat`**. Read counts
were generated using **`htseq-count`**.

When controlling for HSC population (`hscPop`: long- vs. short-term) and RNA 
concentration (`ng_per_ul`) variables, 132 genes were found to be significantly
differentially expressed between knockout groups (`koStatus`: WT vs. BCAP KO).

----- 

# R environment set-up

## Loading packages

I first loaded required packages for data munging, visualization, and 
analysis (these are largely Hadley Wickham libraries, plus some 
**`Bioconductor`** tools).

```{r load_packages, cache=FALSE}
rm(list = ls())
cleanup <- gc(verbose = FALSE)

# Load libraries I'll need here
library(edgeR)
library(limma)
library(biomaRt)
library(ggbiplot)
library(cowplot)
library(sva)

# Load my go-to libraries
library(dplyr)
library(ggplot2)
library(ggthemes)
library(stringr)
library(readr)
library(readxl)
library(reshape2)

# Packages for R markdown stuff
library(knitr)
library(shiny)
```

\  

## Defining functions

Functions for plotting metrics are contained in `metric_qc_functions.R`.

```{r source_functions, cache=FALSE}
source("R/metric_qc_functions.R")
```

This is a function written by Elizabeth Whalen (shared by Michael Mason) that
might come in handy with some steps of the analysis. I modified the function
slightly, such that library sizes are updated and normalization factors are
calculated *after* filtering genes. I also added the option to input gene
annotation information.

```{r whalen_functions}
# Function to build DGEList object, filter genes by keeping only those having % 
# samples with at least N counts, and computes normalization from library sizes
setUpDGEList <- function(countData, geneData = NULL, 
                         filterCount = 1, filterPercentage = 0.1)
{
	d <- DGEList(counts = countData, genes = geneData)
	# d <- calcNormFactors(d) # moved further down
    
	# Filter all genes that do not have at least 'filterCount' counts per 
	# million in at least 'filterPercentage' percent of libraries
	keepRows <- rowSums(round(cpm(d$counts)) >= filterCount) >= 
	    filterPercentage*ncol(countData)
	print(table(keepRows))

	curDGE <- d[keepRows, ]
	
	# James: I've added this change so that library sizes and normalization
	# factors will always be updated/calculated after filtering genes
	
	# reset library sizes
	curDGE$samples$lib.size <- colSums(curDGE$counts)
	
	# calculate normalization factors (effective library size = 
	# lib.size * norm.factor)
	curDGE <- calcNormFactors(curDGE)
	return(curDGE)
}
```

\  

## Loading data

Next, I read counts and metrics data for the project into R, along with sample
annotation for project libraries.

```{r load_data}
# Read CSV file with read counts
countFile <- "data/HMMF2ADXX_combined_counts.csv"
countDat <- read_csv(countFile) # 37991 obs. of  18 variables
# str(countDat)

# Read CSV file with RNAseq/alignment metrics
metricFile <- "data/HMMF2ADXX_combined_metrics.csv"
metricDat <- read_csv(metricFile) # 16 obs. of  71 variables
# str(metricDat)

# Read XLSX file with sample annotation
designFile <- "data/JMD119 Sample Information .xlsx"
designDat <- read_excel(designFile, skip = 1) # 36 obs. of 18 variables
# str(designDat)
```

\  

## Cleaning data

I needed to do a bit of cleaning/formatting with variable names (column 
headers) to make life easier and avoid breaking downstream functions.

```{r clean_data}
# Separate gene counts and gene symbols into separate objects, reformat
# variable names in countDat to only include libID
geneDat <- data_frame(ensemblID = countDat$geneName)
countDat <- countDat %>% 
    select(-geneName)
names(countDat) <- names(countDat) %>% 
    str_extract("lib[0-9]+")

# Reformat variable names in metrics data frame
names(metricDat) <- names(metricDat) %>% 
    str_to_lower() %>%  # change variable names to lower case
    make.unique(sep = "_") # de-dup variable names
names(metricDat)[1] <- "lib_id" # reformat libID variable name

# Reformat row names in metrics dataframe
metricDat <- metricDat %>% 
    mutate(lib_id = str_extract(lib_id, "lib[0-9]+"))

# Reformat variable names in design data frame
names(designDat) <- names(designDat) %>% 
    str_replace_all(" +", "_") %>% # replace spaces with underscores
    str_replace_all("#", "num") %>%  # replace # with 'num'
    str_replace_all("/", "_per_") %>% 
    str_replace_all("(\\(|\\))", "") %>% # remove parentheses
    str_to_lower() %>% # change to lower case
    str_replace("(?<=(lib))[a-z]+", "") %>% # replace 'library' with 'lib'
    make.unique(sep = "_") # de-dup variable names

# Remove empty rows from design data frame
designDat <- designDat %>% 
    filter(!is.na(lib_id))
```

\  

I created a new object to store the salient information about groups in the 
study I want to compare.

```{r get_groups}
groupDat <- designDat %>% 
    # extract knockout status (WT or BCAP) and HSC population (long or short'
    # term); combine into a single group vector
    mutate(koStatus = as.factor(tolower(str_extract(sample_name, "WT|BCAP"))),
           hscPop = as.factor(tolower(str_extract(hsc_population, 
                                                  "Long|Short"))),
           group = as.factor(str_c(koStatus, hscPop, sep = "_"))) %>% 
    select(libID = lib_id,
           koStatus, hscPop, group)
```

For reference, here are the relevant groups in the data (stored in `groupDat`):

`r kable(groupDat, format = "html")`

-----

# Inspecting data

## Metric plotting & QC

Next, I looked at a few standard metrics to see whether any libraries should
be excluded due to quality reasons.

```{r summarize_metrics}
# Pull out and format the subset of metrics to plot
metricSummary <- metricDat %>% 
    mutate(percentDuplication = unpaired_read_duplicates / 
               unpaired_reads_examined) %>% 
    select(libID = lib_id, 
           medianCVcoverage = median_cv_coverage, 
           fastqTotalReads = fastq_total_reads, 
           percentAligned = mapped_reads_w_dups,
           percentDuplication)
```

Cutoffs are set by default to standard values used in the Bioinformatics Core
for three metrics; libraries are considered to have 'failed' QC for the 
following conditions:  

* < 1 million total FASTQ reads  
* < 80% aligned reads  
* \> 1.0 median CV coverage

I can also adjust slider bars to look at different QC cutoffs (red lines) for 
the x- and y-axis; dashed lines indicate outlier limits (1.5*IQR).

\  

### Percent aligned

Percent aligned is simply the number of FASTQ reads for which there is a
corresponding alignment in the **`TopHat`** BAM file. In other words,
`percentAligned` = # of aligned reads (+ all their duplicate reads, which
were removed from the final BAM) / `fastqTotalReads`.

```{r plot_perc_alingned_shiny, echo=FALSE, cache=FALSE}
# Skipping the shiny version for now...
# sliderInput("alignCutoff", "percent aligned cutoff:", 
#             min = 0, max = 1, step = 0.01,
#             value = 0.8)
# sliderInput("readsCutoff", "total FASTQ reads cutoff:", 
#             min = 0, max = 5e7, step = 1e5,
#             value = 1e6)

input <- list(alignCutoff = 0.8,
              readsCutoff = 1e6,
              covCutoff = 1.0)

# renderPlot({
    yRange <-c(input$alignCutoff, 1)
    xRange <- c(input$readsCutoff, max(metricSummary$fastqTotalReads) + 1e6)
    
    metricSummary %>%
        plot_metric("percentAligned", yRange, xRange)
# })
```

***Percent aligned:***  
Percentage of aligned reads is well above the 80% cutoff for all libraries, 
with rates in the mid-90s across the board. **lib7422** is outside the nominal
outlier range, but still has `r filter(metricSummary, libID == "lib7422") %>% 
select(percentAligned) * 100`% alignment.

***Total reads:***  
While all libraries had well over 10 million reads in the input FASTQ file 
(after adapter trimming), **lib7418** appears to be quite a bit smaller than
average the average of 
`r summarise(metricSummary, mean(fastqTotalReads)) / 1e6` million reads, with 
only `r filter(metricSummary, libID == "lib7418") %>% select(fastqTotalReads) /
1e6` million reads.

\  

### Median CV coverage

Median CV coverage is calculated by **`Picard`** by

1. calculating the coeficient of variation (CV) in read coverage along the
length of each of the 1000 most highly expressed transcripts;
2. calculating the median CV across these 1000 transcripts.

A high CV of read coverage suggests possible 5' or 3' bias, or potentially
non-uniform ("bumpy" or "spikey") coverage of a transcript. If 
`medianCVcoverage` is high (> 1), this could indicate a more systemic problem
with coverage in the dataset.

```{r plot_med_cv_cov_shiny, echo=FALSE, cache=FALSE}
# Skipping shiny version for now
# sliderInput("covCutoff", "median CV coverage cutoff:", 
#             min = 0, max = 3, step = 0.01,
#             value = 1)

# renderPlot({
    yRange <- c(0, input$covCutoff)
    xRange <- c(input$readsCutoff, max(metricSummary$fastqTotalReads) + 1e6)
    
    metricSummary %>%
        plot_metric("medianCVcoverage", yRange, xRange)
# })
```

***Median CV coverage:***  
All libraries look good (with `medianCVcoverage` generally close to 0.5) in
terms of gene coverage among the top 1000 transcripts.

\  

## Examining read count data

I plotted the (smoothed) frequency of log-normalized counts in each library,
just to get a sense of the distribution.

```{r raw_count_dists, echo=FALSE, fig.height=4}
countDat %>% 
    melt() %>% 
    mutate(count = value,
           libID = variable) %>% 
    ggplot(aes(x = log2(count + 0.5))) +
    geom_density(aes(colour = libID), alpha = 0.2) +
    theme_classic() +
    scale_color_manual(values = colorRampPalette(colorblind_pal()(8))(16))
```

Without any filtering, there's a large spike at -1, representing genes with a
count of 0, along with a smaller bump between 0 and 1, representing genes with
very small counts. Notably, the distribution of counts for **lib7418** is
shifted dramatically to the left (which makes sense, given the much smaller
number of pre-aligned reads).

\  

### Gene filtering

I used the function defined above to build the `DGEList` object for the data,
which is the input for downstream functions.

```{r build_dge_list}
# Filter genes with (cpm > 10) in < 10% of samples
dge = setUpDGEList(countData = countDat, geneData = geneDat,
                      filterCount = 10, 
                      filterPercentage = 0.20)
```

Keeping only those genes with >= 10 counts per million in at least 20% (
`r 0.2 * nrow(groupDat)` samples), we're left with `r nrow(dge)` genes.

```{r dge_count_dists, echo=FALSE, fig.height=3.5}
dge$counts %>%
    melt() %>% 
    mutate(count = value,
           libID = Var2) %>% 
    ggplot(aes(x = log2(count + 0.5))) +
    geom_density(aes(colour = libID), alpha = 0.2) +
    theme_classic() +
    scale_color_manual(values = colorRampPalette(colorblind_pal()(8))(16))
```

While there are still a small fraction of genes with zero or very few counts, 
these no longer account for such a large percentage of genes across all 
libraries.

```{r dge_cpm_dists, echo=FALSE, fig.height=3.5}
cpm(dge$counts) %>%
    melt() %>% 
    mutate(cpm = value,
           libID = Var2) %>% 
    ggplot(aes(x = log2(cpm + 0.5))) +
    geom_density(aes(colour = libID), alpha = 0.2) +
    theme_classic() +
    scale_color_manual(values = colorRampPalette(colorblind_pal()(8))(16))
```

Correcting for library size, the distribution of counts per million (CPM) is
more similar across all libraries, including **lib7418**.

\  

### Checking code edits

To verify the effect of the change I made to Elizabeth's code above, I plotted
`norm.factors` and effective library size (`lib.size.eff`) under two 
scenarios:

* **preFilter:** `norm.factors` are calculated before genes are filtered
* **postFilter:** `norm.factors` are calculated after genes are filtered and
library sizes are updated

```{r sanity_check, echo=FALSE}
# pre-filter calculation of norm.factors
d1 <- DGEList(countDat)
d1 <- calcNormFactors(d1)

# initialize data frame to store results from both scenarios
normFactorTest <- d1$samples %>% 
    add_rownames(var = "libID") %>% 
    mutate(test = "preFilter") %>% 
    select(-group)

# post-filter calculation of norm.factors
filterCount <- 10
filterPercentage <- 0.2
d2 <- DGEList(countDat)
keepRows <- rowSums(round(cpm(d2$counts)) >= filterCount) >= 
    filterPercentage*ncol(countDat)
d2 <- d2[keepRows, ]
d2$samples$lib.size <- colSums(d2$counts)
d2 <- calcNormFactors(d2)

# update combined data frame
normFactorTest <- d2$samples %>% 
    add_rownames(var = "libID") %>% 
    mutate(test = "postFilter") %>% 
    select(-group) %>% 
    bind_rows(normFactorTest, .)

# plot norm.factors and lib.size.eff for both scenarios
normFactorTest %>% 
    mutate(test = relevel(as.factor(test), "preFilter"),
           lib.size.eff = norm.factors * lib.size) %>% 
    melt(measure.vars = c("norm.factors", "lib.size.eff")) %>% 
    ggplot(aes(x = libID, y = value)) +
    geom_point(aes(fill = test), shape = 21, size = 3,
               colour = "white", alpha = 0.7,
               position = position_jitter(width = 0.1, height = 0)) +
    facet_wrap(~ variable, scales = "free_y") +
    theme_classic() +
    theme(axis.text.x = element_text(angle = 90),
          panel.grid.major = element_line(colour = "grey", linetype = 3)) +
    scale_fill_colorblind()
```

For the not-too-stringent threshold used to filter genes (CPM > 10 in 20% of
samples), the order of operations for calculating `norm.factors` appears to
have minimal impact on effective library sizes.

\  

### Gene annotation

I used the **`biomaRt`** package to add gene symbols (from MGI) corresponding 
to gene IDs from Ensembl.

```{r annotate_genes}
# Get MGI gene symbols corresponding to Ensembl Gene IDs
dgeGeneDat <- dge$genes
mart <- useMart(biomart = "ensembl", dataset = "mmusculus_gene_ensembl")
ens2Gene <- getBM(attributes = c("ensembl_gene_id", "mgi_symbol"), 
                  filters = "ensembl_gene_id", 
                  values = dgeGeneDat$ensemblID, mart = mart)
ens2Gene = ens2Gene[match(dgeGeneDat$ensemblID, ens2Gene$ensembl_gene_id), ]

# Insert MGI gene symbols for genes in DGE object gene info
dgeGeneDat <- dgeGeneDat %>% 
    mutate(mgiSymbol = ens2Gene$mgi_symbol,
           mgiSymbol = ifelse(is.na(mgiSymbol), "NA", mgiSymbol))

dge$genes <- dgeGeneDat
```

\  

### Principal component analysis (PCA)

I performed PCA (on log-normalized CPM values) with the `prcomp` function and 
plotted the first two principal components using the `ggbiplot` function from 
the **`ggbiplot`** package.

```{r pca, echo=FALSE}
# Compute principal components
pca <- prcomp(t(log2(cpm(dge$counts + 0.5))))

# Plot first two PCs and color by group
ggbiplot(pca, choices = c(1,2), var.axes = FALSE,
         groups = groupDat$group, labels = groupDat$libID, labels.size = 3) +
    scale_color_colorblind() +
    guides(size = FALSE) +
    theme_classic() +
    theme(axis.title = element_text(size = 11))

# Plot the percentage of std dev explained by each PC
data_frame(pc = 1:length(pca$sdev), 
           percVariance = pca$sdev^2 / sum(pca$sdev^2)) %>% 
    ggplot(aes(x = pc, y = percVariance)) +
    geom_bar(stat = "identity", alpha = 0.8) +
    theme_classic() +
    scale_fill_colorblind()
```

The second principal component (PC2) appears to explain the difference between
long- and short-term HSC populations -- this is clearly a strong singal in the
complete dataset. PC1, on the other hand, is not obviously correlated with any
experimental group or any of the metrics I examined above.

Among the first two principal components, there is no clear clustering by
KO status.

```{r pca_extra, echo=FALSE, fig.height=6}
# Plot other combinations of PCs
p1 <- ggbiplot(pca, choices = c(1,3), var.axes = FALSE,
               groups = groupDat$group, labels = groupDat$libID, labels.size = 3) +
    scale_color_colorblind() +
    guides(size = FALSE) +
    theme_classic() +
    theme(axis.title = element_text(size = 9))

p2 <- ggbiplot(pca, choices = c(2,3), var.axes = FALSE,
               groups = groupDat$group, labels = groupDat$libID, labels.size = 3) +
    scale_color_colorblind() +
    guides(size = FALSE) +
    theme_classic() +
    theme(axis.title = element_text(size = 9))

p3 <- ggbiplot(pca, choices = c(1,4), var.axes = FALSE,
               groups = groupDat$group, labels = groupDat$libID, labels.size = 3) +
    scale_color_colorblind() +
    guides(size = FALSE) +
    theme_classic() +
    theme(axis.title = element_text(size = 9))

p4 <- ggbiplot(pca, choices = c(2,4), var.axes = FALSE,
               groups = groupDat$group, labels = groupDat$libID, labels.size = 3) +
    scale_color_colorblind() +
    guides(size = FALSE) +
    theme_classic() +
    theme(axis.title = element_text(size = 9))

plot_grid(p1, p2, p3, p4)
```

Looking at some of the other top principal components, PC4 appears to have the
strongest (though not perfect) relationship with KO status. When PC4 is
plotted against PC2 (related to HSC population), the samples separate 
reasonably well into the expected groups. PC3 seems to have almost an inverse 
relationship with PC2.

\  

### Possible sources of confounding

To try to get a better sense of what PC1 might represent in the data, I plotted
several experimental, sequencing, and alignment variables against PC1 values.

```{r plot_pc1, echo=FALSE, fig.height=8}
# Build data frame including design, metric, and other experimental variables
confoundDat <- designDat %>% 
    select(libID = lib_id, sort_date, age_in_weeks, ng_per_ul) %>% 
    left_join(metricSummary, by = c("libID" = "libID")) %>% 
    bind_cols(groupDat %>% select(-libID))

# Include PC1 values with data frame of (potentially) confounding variables
pc1Dat <- bind_cols(confoundDat, melt(pca$x[, 1], value.name = "pc1"))

# Plot PC1 against all other variables
pc1Dat %>% 
    melt(measure.vars = setdiff(names(confoundDat), "libID")) %>%
    ggplot(aes(x = value, y = pc1)) +
    geom_point() +
    facet_wrap(~ variable, nrow = 2, scales = "free_x") +
    stat_smooth(aes(group = 1)) +
    theme_classic() +
    theme(axis.text.x = element_text(angle = 90),
          panel.grid.major = element_line(colour = "grey", linetype = 3)) +
    scale_colour_colorblind()
```

While PC1 does appear to show some relationship with KO status, the two extreme
libraries on the PC1 axis (**lib7418** and **lib7427**) both also have very low
RNA concentrations (`ng_per_ul`). None of the other variables seem to be
particularly associated with PC1.

\  

I also plotted the same set of variables against `koStatus`.

```{r plot_ko_status, echo=FALSE, fig.height=7}
# Plot PC1 against all other variables
confoundDat %>% 
    melt(measure.vars = setdiff(names(confoundDat), 
                                c("libID", "koStatus"))) %>%
    ggplot(aes(x = koStatus, y = value)) +
    geom_point() +
    facet_wrap(~ variable, nrow = 3, scales = "free_y") +
    stat_smooth(aes(group = 1)) +
    theme_classic() +
    theme(axis.text.x = element_text(angle = 90),
          panel.grid.major = element_line(colour = "grey", linetype = 3)) +
    scale_colour_colorblind()
```

None of the variables jump out to me as being obviously correlated with
`koStatus`, with the exception of mice `age_in_weeks`. Because KO mice are all
at least 1/2 week older than WT mice, I don't believe there's any way to 
effectively rule out this variable as being a source of expression changes.
However, I believe the age difference is small enough that the effects are
relatively small.

Despite the slope of some of the fitted lines, none of the RNA-seq quality 
metrics appear to be particularly skewed by `koStatus`.

-----  

# Differential expression analysis

## Building models

***Design 1:*** expression ~ `koStatus`

The first, possibly most naive, model I trained was predicting expression as a
function of BCAP knockout status (i.e., `koStatus`).

```{r ko_voom_weights}
# Define the design matrix, only including terms corresponding to KO status;
# use voom to calculate transformed expression values
koDesign <- model.matrix(~ koStatus, data = groupDat)
koVoom <- voomWithQualityWeights(dge, design = koDesign,
                                    plot = TRUE)
```

Using `voomWithQualityWeights` from the **`limma`** package, several libraries
are downweighted in the normalized data object. The two libraries with the
lowest sample weights are once again those with low `ng_per_ul` values (and
also with extreme PC1 values).

```{r ko_fit_model}
# Fit model for the group design
koFit <- lmFit(koVoom, koDesign)
koFit <- eBayes(koFit)
koResults <- topTable(koFit, number = nrow(dge))
```

This model produces `r nrow(koResults %>% filter(adj.P.Val < 0.05))` significant 
genes:
`r kable(koResults %>% filter(adj.P.Val < 0.05), digits = 3, format = "html")`

`logFC` in this table represents the log fold-change in expression per unit
change in `koStatus` (in other words, the log FC in wild-type relative to KO).

\  

### A note about BCAP

Interestingly, the gene for BCAP (i.e., the knock-out target), **Pik3ap1**, is
not significant after multiple testing correction:
`r kable(koResults %>% filter(mgiSymbol == "Pik3ap1"), digits = 3, format = "html")`

Based on previous inspection of the aligned data in **`IGV`**, the knockout of
BCAP does not manifest entirely cleanly on the RNA level. As seen in the plots
below, BCAP has at least low counts in long-term HSCs and moderate counts in 
short-term HSCs, even in libraries from KO mice.

```{r plot_bcap, echo=FALSE, fig.height=4.5}
# Pull out raw counts for BCAP gene (Pik3ap1)
bcapIdx <- which(koVoom$genes$mgiSymbol == "Pik3ap1")

bcapDatAll <- koVoom$E[bcapIdx, ] %>% 
    melt(value.name = "logCPM") %>% 
    add_rownames(var = "libID") %>% 
    mutate(koStatus = groupDat$koStatus,
           hscPop = groupDat$hscPop,
           samples = "all") %>% 
    bind_rows(koVoom$E[bcapIdx, ] %>% 
                  melt(value.name = "logCPM") %>% 
                  add_rownames(var = "libID") %>% 
                  mutate(koStatus = groupDat$koStatus,
                         hscPop = groupDat$hscPop) %>% 
                  filter(hscPop == "long") %>% 
                  mutate(samples = "long")) %>% 
    bind_rows(koVoom$E[bcapIdx, ] %>% 
                  melt(value.name = "logCPM") %>% 
                  add_rownames(var = "libID") %>% 
                  mutate(koStatus = groupDat$koStatus,
                         hscPop = groupDat$hscPop) %>% 
                  filter(hscPop == "short") %>% 
                  mutate(samples = "short"))

# Plot BCAP counts by KO status and facet by HSC population
bcapDatAll %>% 
    ggplot(aes(x = koStatus, y = logCPM)) +
    geom_boxplot(aes(colour = koStatus)) +
    geom_point(aes(fill = koStatus),
                size = 3, alpha = 0.8, shape = 21, colour = "white") +
    geom_text(aes(label = libID),
                  hjust = -0.2, vjust = 1.2, size = 3) +
    facet_wrap(~ samples) +
    theme_classic() +
    scale_color_colorblind() + 
    scale_fill_colorblind()
```

\  

***Design 2:*** expression ~ `koStatus` + `hscPop`

Because HSC population (i.e., `hscPop`: long- vs. short-term) accounts for such 
a large fraction of variance in the data, it makes sense to control for this 
variable by including it as a parameter in the model. We lose a degree of
freedom in the process, but retain more DoF than if we were to subset the data
and focus only on long- or short-term mice.

```{r ko_hsc_voom_weights}
# Define the design matrix, including terms corresponding to KO status and HSC
# population; use voom to calculate transformed expression values
koHscDesign <- model.matrix(~ koStatus + hscPop, data = groupDat)
koHscVoom <- voomWithQualityWeights(dge, design = koHscDesign,
                                    plot = TRUE)
```

```{r ko_hsc_fit_model}
# Fit model for the group design
koHscFit <- lmFit(koHscVoom, koHscDesign)
koHscFit <- eBayes(koHscFit)
koHscResults <- topTable(koHscFit, coef = 2, number = nrow(dge))
```

While the model includes parameters for both `koStatus` and `hscPop`, we're 
most interested in the effect of `koStatus` on expression. By examining the 
results for the 2nd model coefficient (1st coefficient is the intercept), we
can interpret `logFC` in the table below as the log FC in expression in WT
relative to KO mice, when all other parameters are held constant.

This model produces `r nrow(koHscResults %>% filter(adj.P.Val < 0.05))` 
significant genes for `koStatus`:
`r kable(koHscResults %>% filter(adj.P.Val < 0.05), digits = 3, format = "html")`

\  

**Pik3ap1** is still not significant after multiple testing correction:
`r kable(koHscResults %>% filter(mgiSymbol == "Pik3ap1"), digits = 3, format = "html")`

\  

### Surrogate variable analysis (SVA)

To dig a bit further into PC1 / `ng_per_ul`, which might be introducing
unwanted noise into the model, I identified surrogate variables using
the **`sva`** package. Surrogate variables represent sources of variance in the
data, not accounted for by the primary design variables (i.e., the factors of
interest); SVA is commonly used to identify and remove potential confounding
variables when there may not be clear "batches" among samples.

SVAs were computed on the normalized data with the model matrix for the
`~ koStatus + hscPop` design

As shown in the plots below, PC1, SV1, and also the `voom` sample quality 
weights all appear to be correlated with RNA concentration (`ng_per_ul`).

```{r sva}
koSva <- sva(koHscVoom$E, koHscVoom$design)
```

```{r rna_conc_plot, echo=FALSE, fig.height=3.5}
data_frame(pc1 = pca$x[, 1], sv1 = koSva$sv[, 1], 
           voomQualWts = koHscVoom$sample.weights,
           rnaConc = confoundDat$ng_per_ul) %>% 
    melt(measure.vars = c("sv1", "pc1", "voomQualWts")) %>% 
    ggplot(aes(x = rnaConc, y = value)) +
    geom_point() +
    stat_smooth(method = "lm") +
    facet_wrap(~ variable, scales = "free_y")
```

\  

***Design 3:*** expression ~ `koStatus` + `hscPop` + `ng_per_ul`

To control for RNA concentration, I added the `ng_per_ul` as a parameter in the
model.

```{r ko_hsc_conc_voom_weights}
# Define the design matrix, including terms corresponding to KO status, HSC
# population, and RNA concentration; use voom to calculate transformed 
# expression values
koHscConcDesign <- model.matrix(~ koStatus + hscPop + ng_per_ul, 
                                data = confoundDat)
koHscConcVoom <- voomWithQualityWeights(dge, design = koHscDesign,
                                    plot = TRUE)
```

Notably, `voom` still down-weights the libraries with low `ng_per_ul`, suggesting
that the "poor" quality in these samples is not fully explained by the RNA
concentration variable.

```{r ko_hsc_conc_fit_model}
# Fit model for the group design
koHscConcFit <- lmFit(koHscConcVoom, koHscConcDesign)
koHscConcFit <- eBayes(koHscConcFit)
koHscConcResults <- topTable(koHscConcFit, coef = 2, number = nrow(dge))
```

This new model produces `r nrow(koHscConcResults %>% filter(adj.P.Val < 0.05))` 
significant genes for `koStatus`:
`r kable(koHscConcResults %>% filter(adj.P.Val < 0.05), digits = 3, format = "html")`

\  

**Pik3ap1** is still not significant after multiple testing correction:
`kable(koHscConcResults %>% filter(mgiSymbol == "Pik3ap1"), digits = 3, format = "html")`

\  

***Design 3.5:*** expression ~ `koStatus` + `hscPop` + `ng_per_ul`, no quality
weighting of samples

Because I've accounted for the most obvious source of the variance explained by
PC1 - and because `ng_per_ul` also appears to be correlated with the sample
quality weights from `voom` - I opted to train an additional model without the
quality weighting.

```{r ko_hsc_conc_voom_weights_noqual}
# Define the design matrix, including terms corresponding to KO status, HSC
# population, and RNA concentration; use voom to calculate transformed 
# expression values
koHscConcVoomNoQual <- voom(dge, design = koHscDesign, plot = TRUE)
```

```{r ko_hsc_conc_fit_mode_noqual}
# Fit model for the group design
koHscConcNoQualFit <- lmFit(koHscConcVoomNoQual, koHscConcDesign)
koHscConcNoQualFit <- eBayes(koHscConcNoQualFit)
koHscConcNoQualResults <- topTable(koHscConcNoQualFit, coef = 2, number = nrow(dge))
```

This new model produces 
`r nrow(koHscConcNoQualResults %>% filter(adj.P.Val < 0.05))` 
significant genes for `koStatus`:
`r kable(koHscConcNoQualResults %>% filter(adj.P.Val < 0.05), digits = 3, format = "html")`

\  

**Pik3ap1** is borderline significant after multiple testing correction:
`r kable(koHscConcNoQualResults %>% filter(mgiSymbol == "Pik3ap1"), digits = 3, format = "html")`

While, for the reasons stated above, differential BCAP expression should 
probably not be viewed as a positive control, it's at least somewhat 
encouraging to see the model better account for this difference.

\  

## Model (design) comparison

The plot below shows the full list of genes that were found to be significant
with one or more of the above model designs. For each gene, a colored bar
indicates the design under which it was found to be significant (the size of
the bar is scaled by the adjusted p-value).

The gene lists for designs 1 and 2 overlap completely with designs 3 and 3.5;
there were 4 genes from design 3 that were not significant in design 3.5.

***Genes with significantly different expression (adj. p-value < 0.05) as a
function of BCAP KO status:***

```{r compare_models, echo=FALSE, fig.width=9.5, fig.height=5}
sigGeneDat <- koResults %>% 
    filter(adj.P.Val < 0.05) %>% 
    mutate(design = "design1") %>% 
    bind_rows(koHscResults %>% 
                  filter(adj.P.Val < 0.05) %>% 
                  mutate(design = "design2")) %>% 
    bind_rows(koHscConcResults %>% 
                  filter(adj.P.Val < 0.05) %>% 
                  mutate(design = "design3")) %>% 
    bind_rows(koHscConcNoQualResults %>% 
                  filter(adj.P.Val < 0.05) %>% 
                  mutate(design = "design3.5")) %>% 
    mutate(geneID_symbol = str_c(str_extract(ensemblID, "[1-9].*"), 
                                 mgiSymbol, sep = "_"))

geneLevels <- sigGeneDat %>% 
    group_by(geneID_symbol) %>% 
    summarise(score = n(), meanPval = mean(adj.P.Val)) %>% 
    arrange(desc(score), meanPval) %>% 
    select(geneID_symbol) %>% 
    unlist()

sigGeneDat <- sigGeneDat %>% 
    mutate(geneID_symbol = factor(geneID_symbol, levels = geneLevels))

sigGeneDat %>% 
    ggplot(aes(x = geneID_symbol, y = -log10(adj.P.Val))) +
    geom_bar(aes(fill = design), stat = "identity",
               alpha = 0.8, colour = "white") +
    theme_classic() +
    scale_color_colorblind() + 
    scale_fill_colorblind() +
    theme(axis.text.x = element_text(angle = -90, hjust = 0, size = 6),
          legend.position = "top")
```

----- 

# Examining HSC populations separately

Just to check, I also tested for differential expression with only libraries
from the same HSC population included. For both long- and short-term HSCs,
there were no significant genes for `koStatus`.

## Long-term HSC mice only

```{r build_dge_list_long}
# Get data for libraries from long-term HSC population
groupDatLong <- groupDat %>% 
    filter(hscPop == "long")
countDatLong <- countDat[, names(countDat) %in% groupDatLong$libID]

# Filter genes with (cpm > 10) in < 10% of samples
dgeLong = setUpDGEList(countData = countDatLong, geneData = geneDat,
                      filterCount = 10, 
                      filterPercentage = 0.20)
```

```{r annotate_genes_long, echo = FALSE}
# Get MGI gene symbols corresponding to Ensembl Gene IDs
dgeLongGeneDat <- dgeLong$genes
mart <- useMart(biomart = "ensembl", dataset = "mmusculus_gene_ensembl")
ens2Gene <- getBM(attributes = c("ensembl_gene_id", "mgi_symbol"), 
                  filters = "ensembl_gene_id", 
                  values = dgeLongGeneDat$ensemblID, mart = mart)
ens2Gene = ens2Gene[match(dgeLongGeneDat$ensemblID, ens2Gene$ensembl_gene_id), ]

# Insert MGI gene symbols for genes in DGE object gene info
dgeLongGeneDat <- dgeLongGeneDat %>% 
    mutate(mgiSymbol = ens2Gene$mgi_symbol,
           mgiSymbol = ifelse(is.na(mgiSymbol), "NA", mgiSymbol))

dgeLong$genes <- dgeLongGeneDat
```

```{r plot_pc1_long, echo=FALSE}
# Build data frame including design, metric, and other experimental variables
confoundDatLong <- designDat %>% 
    filter(lib_id %in% groupDatLong$libID) %>% 
    select(libID = lib_id, sort_date, age_in_weeks, ng_per_ul) %>% 
    left_join(metricSummary, by = c("libID" = "libID")) %>% 
    bind_cols(groupDatLong %>% select(-libID))
```

\  

***Design:*** expression ~ `koStatus`

```{r ko_voom_weights_long}
# Define the design matrix, only including terms corresponding to KO status;
# use voom to calculate transformed expression values
koDesignLong <- model.matrix(~ koStatus, data = groupDatLong)
koVoomLong <- voomWithQualityWeights(dgeLong, design = koDesignLong,
                                    plot = TRUE)
```

```{r ko_fit_model_long}
# Fit model for the group design
koFitLong <- lmFit(koVoomLong, koDesignLong)
koFitLong <- eBayes(koFitLong)
koResultsLong <- topTable(koFitLong, number = nrow(dgeLong))
```

This  model produces 
`r nrow(koResultsLong %>% filter(adj.P.Val < 0.05))` 
significant genes for `koStatus`.

\  

## Short-term HSC mice only

```{r build_dge_list_short}
# Get data for libraries from long-term HSC population
groupDatShort <- groupDat %>% 
    filter(hscPop == "short")
countDatShort <- countDat[, names(countDat) %in% groupDatShort$libID]

# Filter genes with (cpm > 10) in < 10% of samples
dgeShort = setUpDGEList(countData = countDatShort, geneData = geneDat,
                      filterCount = 10, 
                      filterPercentage = 0.20)
```

```{r annotate_genes_short, echo=FALSE}
# Get MGI gene symbols corresponding to Ensembl Gene IDs
dgeShortGeneDat <- dgeShort$genes
mart <- useMart(biomart = "ensembl", dataset = "mmusculus_gene_ensembl")
ens2Gene <- getBM(attributes = c("ensembl_gene_id", "mgi_symbol"), 
                  filters = "ensembl_gene_id", 
                  values = dgeShortGeneDat$ensemblID, mart = mart)
ens2Gene = ens2Gene[match(dgeShortGeneDat$ensemblID, ens2Gene$ensembl_gene_id), ]

# Insert MGI gene symbols for genes in DGE object gene info
dgeShortGeneDat <- dgeShortGeneDat %>% 
    mutate(mgiSymbol = ens2Gene$mgi_symbol,
           mgiSymbol = ifelse(is.na(mgiSymbol), "NA", mgiSymbol))

dgeShort$genes <- dgeShortGeneDat
```

```{r plot_pc1_short, echo=FALSE}
# Build data frame including design, metric, and other experimental variables
confoundDatShort <- designDat %>% 
    filter(lib_id %in% groupDatShort$libID) %>% 
    select(libID = lib_id, sort_date, age_in_weeks, ng_per_ul) %>% 
    left_join(metricSummary, by = c("libID" = "libID")) %>% 
    bind_cols(groupDatShort %>% select(-libID))
```

\  

***Design:*** expression ~ `koStatus`

```{r ko_voom_weights_short}
# Define the design matrix, only including terms corresponding to KO status;
# use voom to calculate transformed expression values
koDesignShort <- model.matrix(~ koStatus, data = groupDatShort)
koVoomShort <- voomWithQualityWeights(dgeShort, design = koDesignShort,
                                    plot = TRUE)
```

```{r ko_fit_model_short}
# Fit model for the group design
koFitShort <- lmFit(koVoomShort, koDesignShort)
koFitShort <- eBayes(koFitShort)
koResultsShort <- topTable(koFitShort, number = nrow(dgeShort))
```

This  model produces 
`r nrow(koResultsShort %>% filter(adj.P.Val < 0.05))` 
significant genes for `koStatus`.

----- 

# Session info

```{r session_info}
sessionInfo()
```

